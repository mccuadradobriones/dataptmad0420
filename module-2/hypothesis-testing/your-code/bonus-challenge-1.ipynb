{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bonus Challenge 1 - T-test\n",
    "\n",
    "In statistics, t-test is used to test if two data samples have a significant difference between their means. There are two types of t-test:\n",
    "\n",
    "* **Student's t-test** (a.k.a. independent or uncorrelated t-test). This type of t-test is to compare the samples of **two independent populations** (e.g. test scores of students in two different classes). `scipy` provides the [`ttest_ind`](https://docs.scipy.org/doc/scipy-0.15.1/reference/generated/scipy.stats.ttest_ind.html) method to conduct student's t-test.\n",
    "\n",
    "* **Paired t-test** (a.k.a. dependent or correlated t-test). This type of t-test is to compare the samples of **the same population** (e.g. scores of different tests of students in the same class). `scipy` provides the [`ttest_re`](https://docs.scipy.org/doc/scipy-0.15.1/reference/generated/scipy.stats.ttest_rel.html) method to conduct paired t-test.\n",
    "\n",
    "Both types of t-tests return a number which is called the **p-value**. If p-value is below 0.05, we can confidently declare the null-hypothesis is rejected and the difference is significant. If p-value is between 0.05 and 0.1, we may also declare the null-hypothesis is rejected but we are not highly confident. If p-value is above 0.1 we do not reject the null-hypothesis.\n",
    "\n",
    "Read more about the t-test in [this article](http://b.link/test50) and [this Quora](http://b.link/unpaired97). Make sure you understand when to use which type of t-test. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /home/carmencuadrado/.kaggle/kaggle.json'\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import kaggle\n",
    "\n",
    "from scipy.stats import t\n",
    "from scipy.stats import norm\n",
    "from scipy.stats import ttest_1samp\n",
    "\n",
    "from scipy.stats import ttest_rel, ttest_1samp, ttest_ind\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import dataset\n",
    "\n",
    "In this challenge we will work on the Pokemon dataset you have already used. The goal is to test whether different groups of pokemon (e.g. Legendary vs Normal, Generation 1 vs 2, single-type vs dual-type) have different stats (e.g. HP, Attack, Defense, etc.). Use Ironhack's database to load the data (db: pokemon, table: pokemon_stats). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /home/carmencuadrado/.kaggle/kaggle.json'\n",
      "pokemon.zip: Skipping, found more recently modified local copy (use --force to force download)\n"
     ]
    }
   ],
   "source": [
    "# Your code here:\n",
    "!kaggle datasets download -d abcsds/pokemon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  pokemon.zip\r\n",
      "replace Pokemon.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
     ]
    }
   ],
   "source": [
    "!unzip 'pokemon.zip'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pokemon = pd.read_csv('Pokemon.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "#               0\n",
       "Name            0\n",
       "Type 1          0\n",
       "Type 2        386\n",
       "Total           0\n",
       "HP              0\n",
       "Attack          0\n",
       "Defense         0\n",
       "Sp. Atk         0\n",
       "Sp. Def         0\n",
       "Speed           0\n",
       "Generation      0\n",
       "Legendary       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pokemon.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "L=pokemon[pokemon['Legendary']==True]\n",
    "NL=pokemon[pokemon['Legendary']==False]\n",
    "G1=pokemon[pokemon['Generation']==1]\n",
    "G2=pokemon[pokemon['Generation']==2]\n",
    "Type1=pokemon[~pokemon['Type 2'].notnull()]\n",
    "Type2=pokemon[pokemon['Type 2'].notnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First we want to define a function with which we can test the means of a feature set of two samples. \n",
    "\n",
    "In the next cell you'll see the annotations of the Python function that explains what this function does and its arguments and returned value. This type of annotation is called **docstring** which is a convention used among Python developers. The docstring convention allows developers to write consistent tech documentations for their codes so that others can read. It also allows some websites to automatically parse the docstrings and display user-friendly documentations.\n",
    "\n",
    "Follow the specifications of the docstring and complete the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def t_test_features(s1, s2, features=['HP', 'Attack', 'Defense', 'Sp. Atk', 'Sp. Def', 'Speed', 'Total']):\n",
    "    results=dict()\n",
    "    for i in features:\n",
    "        value=ttest_ind(s1[i], s2[i], equal_var=False)\n",
    "        results[i]=value[1]\n",
    "    \"\"\"Test means of a feature set of two samples\n",
    "    \n",
    "    Args:\n",
    "        s1 (dataframe): sample 1\n",
    "        s2 (dataframe): sample 2\n",
    "        features (list): an array of features to test\n",
    "    \n",
    "    Returns:\n",
    "        dict: a dictionary of t-test scores for each feature where the feature name is the key and the p-value is the value\n",
    "    \"\"\"\n",
    "    \n",
    "    # Your code here    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using the `t_test_features` function, conduct t-test for Lengendary vs non-Legendary pokemons.\n",
    "\n",
    "*Hint: your output should look like below:*\n",
    "\n",
    "```\n",
    "{'HP': 1.0026911708035284e-13,\n",
    " 'Attack': 2.520372449236646e-16,\n",
    " 'Defense': 4.8269984949193316e-11,\n",
    " 'Sp. Atk': 1.5514614112239812e-21,\n",
    " 'Sp. Def': 2.2949327864052826e-15,\n",
    " 'Speed': 1.049016311882451e-18,\n",
    " 'Total': 9.357954335957446e-47}\n",
    " ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The null hypothesis is that the means are equal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'HP': 1.0026911708035284e-13,\n",
       " 'Attack': 2.520372449236646e-16,\n",
       " 'Defense': 4.8269984949193316e-11,\n",
       " 'Sp. Atk': 1.5514614112239812e-21,\n",
       " 'Sp. Def': 2.2949327864052826e-15,\n",
       " 'Speed': 1.049016311882451e-18,\n",
       " 'Total': 9.357954335957446e-47}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Your code here\n",
    "t_test_features(L, NL, features=['HP', 'Attack', 'Defense', 'Sp. Atk', 'Sp. Def', 'Speed', 'Total'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### From the test results above, what conclusion can you make? Do Legendary and non-Legendary pokemons have significantly different stats on each feature?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer\n",
    "\n",
    "As for each and every of the results we have a significance <0.05, we could rejects with a >95% confidence level the null hypothesis. Consequently, we can state that legendary and non-legendary pokemons have different stats on each feature. Legendary pokemon must be more evolved"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Next, conduct t-test for Generation 1 and Generation 2 pokemons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'HP': 0.14551697834219623,\n",
       " 'Attack': 0.24721958967217725,\n",
       " 'Defense': 0.5677711011725426,\n",
       " 'Sp. Atk': 0.12332165977104394,\n",
       " 'Sp. Def': 0.18829872292645752,\n",
       " 'Speed': 0.00239265937312135,\n",
       " 'Total': 0.5631377907941676}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Your code here\n",
    "t_test_features(G1, G2, features=['HP', 'Attack', 'Defense', 'Sp. Atk', 'Sp. Def', 'Speed', 'Total'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What conclusions can you make?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer \n",
    "\n",
    "In all cases, but in \"Speed\", we cannot reject the null hypothesis yielding the conclusion that Generation 1 and Generation 2 pokemons have similar stats. Generation 2 might have experienced a speed improvement vs. Generation 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compare pokemons who have single type vs those having two types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'HP': 0.11314389855379421,\n",
       " 'Attack': 0.00014932578145948305,\n",
       " 'Defense': 2.7978540411514693e-08,\n",
       " 'Sp. Atk': 0.00013876216585667842,\n",
       " 'Sp. Def': 0.00010730610934512779,\n",
       " 'Speed': 0.02421703281819094,\n",
       " 'Total': 1.1157056505229961e-07}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Your code here\n",
    "t_test_features(Type1, Type2, features=['HP', 'Attack', 'Defense', 'Sp. Atk', 'Sp. Def', 'Speed', 'Total'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What conclusions can you make?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer \n",
    "\n",
    "In all cases, but in \"Attack\", \"Sp. Atk\", \"Sp. Def\" and \"Speed\", we can reject the null hypothesis yielding the conclusion that Type 1 and Type 2 pokemons have similar stats. Type 2 might be more prepared for fighting, which could explained its superiority in attack and defence features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now, we want to compare whether there are significant differences of `Attack` vs `Defense`  and  `Sp. Atk` vs `Sp. Def` of all pokemons. Please write your code below.\n",
    "\n",
    "*Hint: are you comparing different populations or the same population?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What conclusions can you make?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def t_test_features(s1, s2):\n",
    "    return ttest_rel(pokemon[s1], pokemon[s2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ttest_relResult(statistic=4.325566393330478, pvalue=1.7140303479358558e-05)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_test_features('Attack', 'Defense')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer \n",
    "\n",
    "According to the low significante obtained from this test, we could reject the null hypothesis and argue that Attack and Defense values divert very much"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ttest_relResult(statistic=0.853986188453353, pvalue=0.3933685997548122)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_test_features('Sp. Atk', 'Sp. Def')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer \n",
    "\n",
    "Regarding the pvalue of this test, we could state that the stats of both features are quite alike to one another"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
