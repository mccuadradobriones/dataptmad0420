{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping Lab\n",
    "\n",
    "You will find in this notebook some scrapy exercises to practise your scraping skills.\n",
    "\n",
    "**Tips:**\n",
    "\n",
    "- Check the response status code for each request to ensure you have obtained the intended content.\n",
    "- Print the response text in each request to understand the kind of info you are getting and its format.\n",
    "- Check for patterns in the response text to extract the data/info requested in each question.\n",
    "- Visit the urls below and take a look at their source code through Chrome DevTools. You'll need to identify the html tags, special class names, etc used in the html content you are expected to extract.\n",
    "\n",
    "**Resources**:\n",
    "- [Requests library](http://docs.python-requests.org/en/master/#the-user-guide)\n",
    "- [Beautiful Soup Doc](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)\n",
    "- [Urllib](https://docs.python.org/3/library/urllib.html#module-urllib)\n",
    "- [re lib](https://docs.python.org/3/library/re.html)\n",
    "- [lxml lib](https://lxml.de/)\n",
    "- [Scrapy](https://scrapy.org/)\n",
    "- [List of HTTP status codes](https://en.wikipedia.org/wiki/List_of_HTTP_status_codes)\n",
    "- [HTML basics](http://www.simplehtmlguide.com/cheatsheet.php)\n",
    "- [CSS basics](https://www.cssbasics.com/#page_start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Below are the libraries and modules you may need. `requests`,  `BeautifulSoup` and `pandas` are already imported for you. If you prefer to use additional libraries feel free to do it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "from urllib.parse import urljoin, urlparse\n",
    "from twitterscraper import query_tweets\n",
    "import datetime as dt\n",
    "from multiprocessing import Pool\n",
    "import time\n",
    "from IPython.display import display\n",
    "import tweepy     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download, parse (using BeautifulSoup), and print the content from the Trending Developers page from GitHub:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://github.com/trending/developers'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "html = requests.get(url).content\n",
    "soup = BeautifulSoup(html, 'lxml')\n",
    "cool_people = soup.find_all('h1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cool_people=re.findall('href.*\\w+.*\\s\\w*\\s.*', str(cool_people))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(jpsim) JP Simard', (frenck) Franck Nijhof', (chrisbanes) Chris Banes', (arvidn) Arvid Norberg', (JedWatson) Jed Watson', (sdushantha) Siddharth Dushantha', (PatilShreyas) Shreyas Patil', (JoviDeCroock) Jovi De Croock', (tmbo) Tom Bocklisch', (PySimpleGUI) PySimpleGUI', (Koenkk) Koen Kanters', (antfu) Anthony Fu', (balloob) Paulus Schoutsen', (casperstorm) Casper Rogild Storm', (compnerd) Saleem Abdulrasool', (tiangolo) Sebastián Ramírez', (mikepenz) Mike Penz', (jsuarezruiz) Javier Suárez', (Mytherin) Mark', (ornicar) Thibault Duplessis', (mpariente) Pariente Manuel', (avelino) Avelino', (a8m) Ariel Mashraki', (dbrgn) Danilo Bargen', (JoshuaKGoldberg) Josh Goldberg']\n"
     ]
    }
   ],
   "source": [
    "cool_people_1=re.sub(r'\">\\\\n',')',str(cool_people))   \n",
    "cool_people_2=re.sub('\\'href=\"/','(',str(cool_people_1))\n",
    "cool_people_3=re.sub('[\\s]{2,20}',' ',str(cool_people_2))\n",
    "print(cool_people_3)\n",
    "consumer_key = \"8G6WNz5F1Qt55hEbPN02Py5ST\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display the names of the trending developers retrieved in the previous step.\n",
    "\n",
    "Your output should be a Python list of developer names. Each name should not contain any html tag.\n",
    "\n",
    "**Instructions:**\n",
    "\n",
    "1. Find out the html tag and class names used for the developer names. You can achieve this using Chrome DevTools.\n",
    "\n",
    "1. Use BeautifulSoup to extract all the html elements that contain the developer names.\n",
    "\n",
    "1. Use string manipulation techniques to replace whitespaces and linebreaks (i.e. `\\n`) in the *text* of each html element. Use a list to store the clean names.\n",
    "\n",
    "1. Print the list of names.\n",
    "\n",
    "Your output should look like below:\n",
    "\n",
    "```\n",
    "['trimstray (@trimstray)',\n",
    " 'joewalnes (JoeWalnes)',\n",
    " 'charlax (Charles-AxelDein)',\n",
    " 'ForrestKnight (ForrestKnight)',\n",
    " 'revery-ui (revery-ui)',\n",
    " 'alibaba (Alibaba)',\n",
    " 'Microsoft (Microsoft)',\n",
    " 'github (GitHub)',\n",
    " 'facebook (Facebook)',\n",
    " 'boazsegev (Bo)',\n",
    " 'google (Google)',\n",
    " 'cloudfetch',\n",
    " 'sindresorhus (SindreSorhus)',\n",
    " 'tensorflow',\n",
    " 'apache (TheApacheSoftwareFoundation)',\n",
    " 'DevonCrawford (DevonCrawford)',\n",
    " 'ARMmbed (ArmMbed)',\n",
    " 'vuejs (vuejs)',\n",
    " 'fastai (fast.ai)',\n",
    " 'QiShaoXuan (Qi)',\n",
    " 'joelparkerhenderson (JoelParkerHenderson)',\n",
    " 'torvalds (LinusTorvalds)',\n",
    " 'CyC2018',\n",
    " 'komeiji-satori (神楽坂覚々)',\n",
    " 'script-8']\n",
    " ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display the trending Python repositories in GitHub.\n",
    "\n",
    "The steps to solve this problem is similar to the previous one except that you need to find out the repository names instead of developer names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://github.com/trending/python?since=daily'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' facebookresearch',\n",
       " ' geohot',\n",
       " ' donnemartin',\n",
       " ' guardicore',\n",
       " ' TheAlgorithms',\n",
       " ' sherlock',\n",
       " ' jofpin',\n",
       " ' scikit',\n",
       " ' aristocratos',\n",
       " ' home',\n",
       " ' timgrossmann',\n",
       " ' gordicaleksa',\n",
       " ' ankitects',\n",
       " ' openai',\n",
       " ' kyb3r',\n",
       " ' QUANTAXIS',\n",
       " ' Rapptz',\n",
       " ' thenewboston',\n",
       " ' scrapy',\n",
       " ' kivy',\n",
       " ' CorentinJ',\n",
       " ' mementum',\n",
       " ' OctoPrint',\n",
       " ' tiangolo',\n",
       " ' lazyprogrammer']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# your code here\n",
    "html = requests.get(url).content\n",
    "soup = BeautifulSoup(html, 'lxml')\n",
    "cool_repos = soup.find_all('h1')\n",
    "cool_repos=re.findall('class=\"text-normal\">\\n.*', str(cool_repos))\n",
    "re.findall(r'(\\s\\b\\w*)',str(cool_repos))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display all the image links from Walt Disney wikipedia page.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://en.wikipedia.org/wiki/Walt_Disney'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "html = requests.get(url).content\n",
    "soup = BeautifulSoup(html, 'lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 34/34 [00:00<00:00, 59025.80it/s]\n"
     ]
    }
   ],
   "source": [
    "urls = []\n",
    "for img in tqdm(soup.find_all('img')):\n",
    "    img_url = img.attrs.get(\"src\")\n",
    "    urls.append(urljoin(url, img_url))\n",
    "    if not img_url:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://upload.wikimedia.org/wikipedia/en/thumb/e/e7/Cscr-featured.svg/20px-Cscr-featured.svg.png',\n",
       " 'https://upload.wikimedia.org/wikipedia/en/thumb/8/8c/Extended-protection-shackle.svg/20px-Extended-protection-shackle.svg.png',\n",
       " 'https://upload.wikimedia.org/wikipedia/commons/thumb/d/df/Walt_Disney_1946.JPG/220px-Walt_Disney_1946.JPG',\n",
       " 'https://upload.wikimedia.org/wikipedia/commons/thumb/8/87/Walt_Disney_1942_signature.svg/150px-Walt_Disney_1942_signature.svg.png',\n",
       " 'https://upload.wikimedia.org/wikipedia/commons/thumb/c/c4/Walt_Disney_envelope_ca._1921.jpg/220px-Walt_Disney_envelope_ca._1921.jpg',\n",
       " 'https://upload.wikimedia.org/wikipedia/commons/thumb/4/4d/Newman_Laugh-O-Gram_%281921%29.webm/220px-seek%3D2-Newman_Laugh-O-Gram_%281921%29.webm.jpg',\n",
       " 'https://upload.wikimedia.org/wikipedia/commons/thumb/0/0d/Trolley_Troubles_poster.jpg/170px-Trolley_Troubles_poster.jpg',\n",
       " 'https://upload.wikimedia.org/wikipedia/commons/thumb/7/71/Walt_Disney_and_his_cartoon_creation_%22Mickey_Mouse%22_-_National_Board_of_Review_Magazine.jpg/170px-Walt_Disney_and_his_cartoon_creation_%22Mickey_Mouse%22_-_National_Board_of_Review_Magazine.jpg',\n",
       " 'https://upload.wikimedia.org/wikipedia/en/thumb/4/4e/Steamboat-willie.jpg/170px-Steamboat-willie.jpg',\n",
       " 'https://upload.wikimedia.org/wikipedia/commons/thumb/5/57/Walt_Disney_1935.jpg/170px-Walt_Disney_1935.jpg',\n",
       " 'https://upload.wikimedia.org/wikipedia/commons/thumb/c/cd/Walt_Disney_Snow_white_1937_trailer_screenshot_%2813%29.jpg/220px-Walt_Disney_Snow_white_1937_trailer_screenshot_%2813%29.jpg',\n",
       " 'https://upload.wikimedia.org/wikipedia/commons/thumb/1/15/Disney_drawing_goofy.jpg/170px-Disney_drawing_goofy.jpg',\n",
       " 'https://upload.wikimedia.org/wikipedia/commons/thumb/1/13/DisneySchiphol1951.jpg/220px-DisneySchiphol1951.jpg',\n",
       " 'https://upload.wikimedia.org/wikipedia/commons/thumb/8/8c/WaltDisneyplansDisneylandDec1954.jpg/220px-WaltDisneyplansDisneylandDec1954.jpg',\n",
       " 'https://upload.wikimedia.org/wikipedia/commons/thumb/f/ff/Walt_disney_portrait_right.jpg/170px-Walt_disney_portrait_right.jpg',\n",
       " 'https://upload.wikimedia.org/wikipedia/commons/thumb/1/1a/Walt_Disney_Grave.JPG/170px-Walt_Disney_Grave.JPG',\n",
       " 'https://upload.wikimedia.org/wikipedia/commons/thumb/2/2d/Roy_O._Disney_with_Company_at_Press_Conference.jpg/170px-Roy_O._Disney_with_Company_at_Press_Conference.jpg',\n",
       " 'https://upload.wikimedia.org/wikipedia/commons/thumb/a/a9/Disney_Display_Case.JPG/170px-Disney_Display_Case.JPG',\n",
       " 'https://upload.wikimedia.org/wikipedia/commons/thumb/6/6c/Disney1968.jpg/170px-Disney1968.jpg',\n",
       " 'https://upload.wikimedia.org/wikipedia/en/thumb/8/8a/OOjs_UI_icon_edit-ltr-progressive.svg/10px-OOjs_UI_icon_edit-ltr-progressive.svg.png',\n",
       " 'https://upload.wikimedia.org/wikipedia/commons/thumb/d/da/Animation_disc.svg/30px-Animation_disc.svg.png',\n",
       " 'https://upload.wikimedia.org/wikipedia/en/thumb/6/69/P_vip.svg/29px-P_vip.svg.png',\n",
       " 'https://upload.wikimedia.org/wikipedia/commons/thumb/1/1a/Magic_Kingdom_castle.jpg/24px-Magic_Kingdom_castle.jpg',\n",
       " 'https://upload.wikimedia.org/wikipedia/en/thumb/e/e7/Video-x-generic.svg/30px-Video-x-generic.svg.png',\n",
       " 'https://upload.wikimedia.org/wikipedia/commons/thumb/a/a3/Flag_of_Los_Angeles_County%2C_California.svg/30px-Flag_of_Los_Angeles_County%2C_California.svg.png',\n",
       " 'https://upload.wikimedia.org/wikipedia/commons/thumb/8/8c/Blank_television_set.svg/30px-Blank_television_set.svg.png',\n",
       " 'https://upload.wikimedia.org/wikipedia/en/thumb/a/a4/Flag_of_the_United_States.svg/30px-Flag_of_the_United_States.svg.png',\n",
       " 'https://upload.wikimedia.org/wikipedia/en/thumb/4/4a/Commons-logo.svg/22px-Commons-logo.svg.png',\n",
       " 'https://upload.wikimedia.org/wikipedia/commons/thumb/f/fa/Wikiquote-logo.svg/25px-Wikiquote-logo.svg.png',\n",
       " 'https://upload.wikimedia.org/wikipedia/commons/thumb/f/ff/Wikidata-logo.svg/30px-Wikidata-logo.svg.png',\n",
       " 'https://upload.wikimedia.org/wikipedia/en/thumb/8/8a/OOjs_UI_icon_edit-ltr-progressive.svg/10px-OOjs_UI_icon_edit-ltr-progressive.svg.png',\n",
       " 'https://en.wikipedia.org/wiki/Special:CentralAutoLogin/start?type=1x1',\n",
       " 'https://en.wikipedia.org/static/images/footer/wikimedia-button.png',\n",
       " 'https://en.wikipedia.org/static/images/footer/poweredby_mediawiki_88x31.png']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "urls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retrieve an arbitary Wikipedia page of \"Python\" and create a list of links on that page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url ='https://en.wikipedia.org/wiki/Python' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "html = requests.get(url).content\n",
    "soup = BeautifulSoup(html, 'lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 152/152 [00:00<00:00, 48637.03it/s]\n"
     ]
    }
   ],
   "source": [
    "urls = []\n",
    "for link in tqdm(soup.find_all('a')):\n",
    "    link_url = link.attrs.get(\"href\")\n",
    "    urls.append(urljoin(url, link_url))\n",
    "    if not link_url:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://en.wikipedia.org/wiki/Python',\n",
       " 'https://en.wikipedia.org/wiki/Python#mw-head',\n",
       " 'https://en.wikipedia.org/wiki/Python#searchInput',\n",
       " 'https://en.wiktionary.org/wiki/Python',\n",
       " 'https://en.wiktionary.org/wiki/python',\n",
       " 'https://en.wikipedia.org/wiki/Pythons',\n",
       " 'https://en.wikipedia.org/wiki/Python_(genus)',\n",
       " 'https://en.wikipedia.org/wiki/Python#Computing',\n",
       " 'https://en.wikipedia.org/wiki/Python#People',\n",
       " 'https://en.wikipedia.org/wiki/Python#Roller_coasters',\n",
       " 'https://en.wikipedia.org/wiki/Python#Vehicles',\n",
       " 'https://en.wikipedia.org/wiki/Python#Weaponry',\n",
       " 'https://en.wikipedia.org/wiki/Python#Other_uses',\n",
       " 'https://en.wikipedia.org/wiki/Python#See_also',\n",
       " 'https://en.wikipedia.org/w/index.php?title=Python&action=edit&section=1',\n",
       " 'https://en.wikipedia.org/wiki/Python_(programming_language)',\n",
       " 'https://en.wikipedia.org/wiki/CMU_Common_Lisp',\n",
       " 'https://en.wikipedia.org/wiki/PERQ#PERQ_3',\n",
       " 'https://en.wikipedia.org/w/index.php?title=Python&action=edit&section=2',\n",
       " 'https://en.wikipedia.org/wiki/Python_of_Aenus',\n",
       " 'https://en.wikipedia.org/wiki/Python_(painter)',\n",
       " 'https://en.wikipedia.org/wiki/Python_of_Byzantium',\n",
       " 'https://en.wikipedia.org/wiki/Python_of_Catana',\n",
       " 'https://en.wikipedia.org/wiki/Python_Anghelo',\n",
       " 'https://en.wikipedia.org/w/index.php?title=Python&action=edit&section=3',\n",
       " 'https://en.wikipedia.org/wiki/Python_(Efteling)',\n",
       " 'https://en.wikipedia.org/wiki/Python_(Busch_Gardens_Tampa_Bay)',\n",
       " 'https://en.wikipedia.org/wiki/Python_(Coney_Island,_Cincinnati,_Ohio)',\n",
       " 'https://en.wikipedia.org/w/index.php?title=Python&action=edit&section=4',\n",
       " 'https://en.wikipedia.org/wiki/Python_(automobile_maker)',\n",
       " 'https://en.wikipedia.org/wiki/Python_(Ford_prototype)',\n",
       " 'https://en.wikipedia.org/w/index.php?title=Python&action=edit&section=5',\n",
       " 'https://en.wikipedia.org/wiki/Python_(missile)',\n",
       " 'https://en.wikipedia.org/wiki/Python_(nuclear_primary)',\n",
       " 'https://en.wikipedia.org/wiki/Colt_Python',\n",
       " 'https://en.wikipedia.org/w/index.php?title=Python&action=edit&section=6',\n",
       " 'https://en.wikipedia.org/wiki/PYTHON',\n",
       " 'https://en.wikipedia.org/wiki/Python_(film)',\n",
       " 'https://en.wikipedia.org/wiki/Python_(mythology)',\n",
       " 'https://en.wikipedia.org/wiki/Monty_Python',\n",
       " 'https://en.wikipedia.org/wiki/Python_(Monty)_Pictures',\n",
       " 'https://en.wikipedia.org/w/index.php?title=Python&action=edit&section=7',\n",
       " 'https://en.wikipedia.org/wiki/Cython',\n",
       " 'https://en.wikipedia.org/wiki/Pyton',\n",
       " 'https://en.wikipedia.org/wiki/Pithon',\n",
       " 'https://en.wikipedia.org/wiki/File:Disambig_gray.svg',\n",
       " 'https://en.wikipedia.org/wiki/Help:Disambiguation',\n",
       " 'https://en.wikipedia.org/w/index.php?title=Special:WhatLinksHere/Python&namespace=0',\n",
       " 'https://en.wikipedia.org/w/index.php?title=Python&oldid=987482924',\n",
       " 'https://en.wikipedia.org/wiki/Help:Category',\n",
       " 'https://en.wikipedia.org/wiki/Category:Disambiguation_pages',\n",
       " 'https://en.wikipedia.org/wiki/Category:Human_name_disambiguation_pages',\n",
       " 'https://en.wikipedia.org/wiki/Category:Disambiguation_pages_with_given-name-holder_lists',\n",
       " 'https://en.wikipedia.org/wiki/Category:Disambiguation_pages_with_short_descriptions',\n",
       " 'https://en.wikipedia.org/wiki/Category:Short_description_is_different_from_Wikidata',\n",
       " 'https://en.wikipedia.org/wiki/Category:All_article_disambiguation_pages',\n",
       " 'https://en.wikipedia.org/wiki/Category:All_disambiguation_pages',\n",
       " 'https://en.wikipedia.org/wiki/Category:Animal_common_name_disambiguation_pages',\n",
       " 'https://en.wikipedia.org/wiki/Special:MyTalk',\n",
       " 'https://en.wikipedia.org/wiki/Special:MyContributions',\n",
       " 'https://en.wikipedia.org/w/index.php?title=Special:CreateAccount&returnto=Python',\n",
       " 'https://en.wikipedia.org/w/index.php?title=Special:UserLogin&returnto=Python',\n",
       " 'https://en.wikipedia.org/wiki/Python',\n",
       " 'https://en.wikipedia.org/wiki/Talk:Python',\n",
       " 'https://en.wikipedia.org/wiki/Python',\n",
       " 'https://en.wikipedia.org/w/index.php?title=Python&action=edit',\n",
       " 'https://en.wikipedia.org/w/index.php?title=Python&action=history',\n",
       " 'https://en.wikipedia.org/wiki/Main_Page',\n",
       " 'https://en.wikipedia.org/wiki/Main_Page',\n",
       " 'https://en.wikipedia.org/wiki/Wikipedia:Contents',\n",
       " 'https://en.wikipedia.org/wiki/Portal:Current_events',\n",
       " 'https://en.wikipedia.org/wiki/Special:Random',\n",
       " 'https://en.wikipedia.org/wiki/Wikipedia:About',\n",
       " 'https://en.wikipedia.org/wiki/Wikipedia:Contact_us',\n",
       " 'https://donate.wikimedia.org/wiki/Special:FundraiserRedirector?utm_source=donate&utm_medium=sidebar&utm_campaign=C13_en.wikipedia.org&uselang=en',\n",
       " 'https://en.wikipedia.org/wiki/Help:Contents',\n",
       " 'https://en.wikipedia.org/wiki/Help:Introduction',\n",
       " 'https://en.wikipedia.org/wiki/Wikipedia:Community_portal',\n",
       " 'https://en.wikipedia.org/wiki/Special:RecentChanges',\n",
       " 'https://en.wikipedia.org/wiki/Wikipedia:File_Upload_Wizard',\n",
       " 'https://en.wikipedia.org/wiki/Special:WhatLinksHere/Python',\n",
       " 'https://en.wikipedia.org/wiki/Special:RecentChangesLinked/Python',\n",
       " 'https://en.wikipedia.org/wiki/Wikipedia:File_Upload_Wizard',\n",
       " 'https://en.wikipedia.org/wiki/Special:SpecialPages',\n",
       " 'https://en.wikipedia.org/w/index.php?title=Python&oldid=987482924',\n",
       " 'https://en.wikipedia.org/w/index.php?title=Python&action=info',\n",
       " 'https://en.wikipedia.org/w/index.php?title=Special:CiteThisPage&page=Python&id=987482924&wpFormIdentifier=titleform',\n",
       " 'https://www.wikidata.org/wiki/Special:EntityPage/Q747452',\n",
       " 'https://en.wikipedia.org/w/index.php?title=Special:DownloadAsPdf&page=Python&action=show-download-screen',\n",
       " 'https://en.wikipedia.org/w/index.php?title=Python&printable=yes',\n",
       " 'https://commons.wikimedia.org/wiki/Category:Python',\n",
       " 'https://af.wikipedia.org/wiki/Python',\n",
       " 'https://als.wikipedia.org/wiki/Python',\n",
       " 'https://ar.wikipedia.org/wiki/%D8%A8%D8%A7%D9%8A%D8%AB%D9%88%D9%86_(%D8%AA%D9%88%D8%B6%D9%8A%D8%AD)',\n",
       " 'https://az.wikipedia.org/wiki/Python',\n",
       " 'https://bn.wikipedia.org/wiki/%E0%A6%AA%E0%A6%BE%E0%A6%87%E0%A6%A5%E0%A6%A8_(%E0%A6%A6%E0%A7%8D%E0%A6%AC%E0%A7%8D%E0%A6%AF%E0%A6%B0%E0%A7%8D%E0%A6%A5%E0%A6%A4%E0%A6%BE_%E0%A6%A8%E0%A6%BF%E0%A6%B0%E0%A6%B8%E0%A6%A8)',\n",
       " 'https://be.wikipedia.org/wiki/Python',\n",
       " 'https://bg.wikipedia.org/wiki/%D0%9F%D0%B8%D1%82%D0%BE%D0%BD_(%D0%BF%D0%BE%D1%8F%D1%81%D0%BD%D0%B5%D0%BD%D0%B8%D0%B5)',\n",
       " 'https://cs.wikipedia.org/wiki/Python_(rozcestn%C3%ADk)',\n",
       " 'https://da.wikipedia.org/wiki/Python',\n",
       " 'https://de.wikipedia.org/wiki/Python',\n",
       " 'https://eo.wikipedia.org/wiki/Pitono_(apartigilo)',\n",
       " 'https://eu.wikipedia.org/wiki/Python_(argipena)',\n",
       " 'https://fa.wikipedia.org/wiki/%D9%BE%D8%A7%DB%8C%D8%AA%D9%88%D9%86',\n",
       " 'https://fr.wikipedia.org/wiki/Python',\n",
       " 'https://ko.wikipedia.org/wiki/%ED%8C%8C%EC%9D%B4%EC%84%A0',\n",
       " 'https://hr.wikipedia.org/wiki/Python_(razdvojba)',\n",
       " 'https://io.wikipedia.org/wiki/Pitono',\n",
       " 'https://id.wikipedia.org/wiki/Python',\n",
       " 'https://ia.wikipedia.org/wiki/Python_(disambiguation)',\n",
       " 'https://is.wikipedia.org/wiki/Python_(a%C3%B0greining)',\n",
       " 'https://it.wikipedia.org/wiki/Python_(disambigua)',\n",
       " 'https://he.wikipedia.org/wiki/%D7%A4%D7%99%D7%AA%D7%95%D7%9F',\n",
       " 'https://ka.wikipedia.org/wiki/%E1%83%9E%E1%83%98%E1%83%97%E1%83%9D%E1%83%9C%E1%83%98_(%E1%83%9B%E1%83%A0%E1%83%90%E1%83%95%E1%83%90%E1%83%9A%E1%83%9B%E1%83%9C%E1%83%98%E1%83%A8%E1%83%95%E1%83%9C%E1%83%94%E1%83%9A%E1%83%9D%E1%83%95%E1%83%90%E1%83%9C%E1%83%98)',\n",
       " 'https://kg.wikipedia.org/wiki/Mboma_(nyoka)',\n",
       " 'https://la.wikipedia.org/wiki/Python_(discretiva)',\n",
       " 'https://lb.wikipedia.org/wiki/Python',\n",
       " 'https://hu.wikipedia.org/wiki/Python_(egy%C3%A9rtelm%C5%B1s%C3%ADt%C5%91_lap)',\n",
       " 'https://mr.wikipedia.org/wiki/%E0%A4%AA%E0%A4%BE%E0%A4%AF%E0%A4%A5%E0%A5%89%E0%A4%A8_(%E0%A4%86%E0%A4%9C%E0%A5%8D%E0%A4%9E%E0%A4%BE%E0%A4%B5%E0%A4%B2%E0%A5%80_%E0%A4%AD%E0%A4%BE%E0%A4%B7%E0%A4%BE)',\n",
       " 'https://nl.wikipedia.org/wiki/Python',\n",
       " 'https://ja.wikipedia.org/wiki/%E3%83%91%E3%82%A4%E3%82%BD%E3%83%B3',\n",
       " 'https://no.wikipedia.org/wiki/Pyton',\n",
       " 'https://pl.wikipedia.org/wiki/Pyton',\n",
       " 'https://pt.wikipedia.org/wiki/Python_(desambigua%C3%A7%C3%A3o)',\n",
       " 'https://ru.wikipedia.org/wiki/Python_(%D0%B7%D0%BD%D0%B0%D1%87%D0%B5%D0%BD%D0%B8%D1%8F)',\n",
       " 'https://sk.wikipedia.org/wiki/Python',\n",
       " 'https://sr.wikipedia.org/wiki/%D0%9F%D0%B8%D1%82%D0%BE%D0%BD_(%D0%B2%D0%B8%D1%88%D0%B5%D0%B7%D0%BD%D0%B0%D1%87%D0%BD%D0%B0_%D0%BE%D0%B4%D1%80%D0%B5%D0%B4%D0%BD%D0%B8%D1%86%D0%B0)',\n",
       " 'https://sh.wikipedia.org/wiki/Python',\n",
       " 'https://fi.wikipedia.org/wiki/Python',\n",
       " 'https://sv.wikipedia.org/wiki/Pyton',\n",
       " 'https://th.wikipedia.org/wiki/%E0%B9%84%E0%B8%9E%E0%B8%97%E0%B8%AD%E0%B8%99',\n",
       " 'https://tr.wikipedia.org/wiki/Python',\n",
       " 'https://uk.wikipedia.org/wiki/%D0%9F%D1%96%D1%84%D0%BE%D0%BD',\n",
       " 'https://ur.wikipedia.org/wiki/%D9%BE%D8%A7%D8%A6%DB%8C%D8%AA%DA%BE%D9%88%D9%86',\n",
       " 'https://vi.wikipedia.org/wiki/Python',\n",
       " 'https://zh.wikipedia.org/wiki/Python_(%E6%B6%88%E6%AD%A7%E4%B9%89)',\n",
       " 'https://www.wikidata.org/wiki/Special:EntityPage/Q747452#sitelinks-wikipedia',\n",
       " 'https://en.wikipedia.org/wiki/Wikipedia:Text_of_Creative_Commons_Attribution-ShareAlike_3.0_Unported_License',\n",
       " 'https://creativecommons.org/licenses/by-sa/3.0/',\n",
       " 'https://foundation.wikimedia.org/wiki/Terms_of_Use',\n",
       " 'https://foundation.wikimedia.org/wiki/Privacy_policy',\n",
       " 'https://www.wikimediafoundation.org/',\n",
       " 'https://foundation.wikimedia.org/wiki/Privacy_policy',\n",
       " 'https://en.wikipedia.org/wiki/Wikipedia:About',\n",
       " 'https://en.wikipedia.org/wiki/Wikipedia:General_disclaimer',\n",
       " 'https://en.wikipedia.org/wiki/Wikipedia:Contact_us',\n",
       " 'https://en.m.wikipedia.org/w/index.php?title=Python&mobileaction=toggle_view_mobile',\n",
       " 'https://www.mediawiki.org/wiki/Special:MyLanguage/How_to_contribute',\n",
       " 'https://stats.wikimedia.org/#/en.wikipedia.org',\n",
       " 'https://foundation.wikimedia.org/wiki/Cookie_statement',\n",
       " 'https://wikimediafoundation.org/',\n",
       " 'https://www.mediawiki.org/']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "urls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find the number of titles that have changed in the United States Code since its last release point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'http://uscode.house.gov/download/download.shtml'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "html = requests.get(url).content\n",
    "soup = BeautifulSoup(html, 'lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 306/306 [00:00<00:00, 184378.25it/s]\n"
     ]
    }
   ],
   "source": [
    "# your code here\n",
    "urls = []\n",
    "for title in tqdm(soup.find_all(\"div\")):\n",
    "    title_name = title.attrs.get(\"class\")\n",
    "    if title_name==['usctitlechanged']:\n",
    "        title=re.sub('(\\<.*?\\>)',\"\",str(title))\n",
    "        title=re.sub('\\\\n',\"\",str(title))\n",
    "        title=title.strip()\n",
    "        urls.append(title)\n",
    "    if not title_name:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title 8 - Aliens and Nationality\n",
      "Title 11 - Bankruptcy ٭\n",
      "Title 15 - Commerce and Trade\n",
      "Title 18 - Crimes and Criminal Procedure ٭\n",
      "Title 25 - Indians\n",
      "Title 31 - Money and Finance ٭\n",
      "Title 32 - National Guard ٭\n",
      "Title 38 - Veterans' Benefits ٭\n",
      "Title 42 - The Public Health and Welfare\n",
      "Title 47 - Telecommunications\n",
      "Title 51 - National and Commercial Space Programs ٭\n"
     ]
    }
   ],
   "source": [
    "for i in urls:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find a Python list with the top ten FBI's Most Wanted names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://www.fbi.gov/wanted/topten'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "html = requests.get(url).content\n",
    "soup = BeautifulSoup(html, 'lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11/11 [00:00<00:00, 40154.35it/s]\n"
     ]
    }
   ],
   "source": [
    "urls = []\n",
    "for img in tqdm(soup.find_all('img')):\n",
    "    img_url = img.attrs.get(\"alt\")\n",
    "    urls.append(img_url)\n",
    "    if not img_url:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ARNOLDO JIMENEZ',\n",
       " 'JASON DEREK BROWN',\n",
       " 'ALEXIS FLORES',\n",
       " 'JOSE RODOLFO VILLARREAL-HERNANDEZ',\n",
       " 'EUGENE PALMER',\n",
       " 'RAFAEL CARO-QUINTERO',\n",
       " 'ROBERT WILLIAM FISHER',\n",
       " 'BHADRESHKUMAR CHETANBHAI PATEL',\n",
       " 'ALEJANDRO ROSALES CASTILLO',\n",
       " 'YASER ABDEL SAID']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "urls[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Display the 20 latest earthquakes info (date, time, latitude, longitude and region name) by the EMSC as a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://www.emsc-csem.org/Earthquake/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "html = requests.get(url).content\n",
    "soup = BeautifulSoup(html, 'lxml')\n",
    "datetime=soup.find_all(\"td\", class_=\"tabev6\")\n",
    "latlon=soup.find_all(\"td\", class_=\"tabev1\")\n",
    "ll=soup.find_all(\"td\", class_=\"tabev2\")\n",
    "region=soup.find_all(\"td\", class_=\"tb_region\")\n",
    "consumer_secret = \"BTZFpOL4uFvmiv4cqorsOnlFjFBF4xfKTAbHoL037ITKRYweji\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "latlon=[re.sub('<td class=\"tabev1\">','',str(i)) for i in latlon]\n",
    "latlon=[re.sub('</td>','',str(i)) for i in latlon]\n",
    "latlon=[re.sub('\\xa0','',str(i)) for i in latlon]\n",
    "latlon =[latlon[i:i + 2] for i in range(0, len(latlon), 2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "ll=[re.sub('<td class=\"tabev2\">','',str(i)) for i in ll]\n",
    "ll=[re.sub('</td>','',str(i)) for i in ll]\n",
    "ll=[re.sub('[\\d]+','',str(i)) for i in ll]\n",
    "ll=[re.sub('\\.','',str(i)) for i in ll]\n",
    "ll=[i[:-2] for i in ll if i]\n",
    "ll =[ll[i:i + 2] for i in range(0, len(ll), 2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "latitud=[]\n",
    "longitud=[]\n",
    "for i,y in zip(latlon,ll):\n",
    "    latitud.append(i[0]+' '+y[0])\n",
    "    longitud.append(i[1]+' '+y[1])\n",
    "access_token = \"1252634486346522626-St00Pi2ftwI0XHHWDtJ769GkLPElw0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "datetime=[re.findall('20[\\d+].*</a>',str(i)) for i in datetime]\n",
    "datetime=[re.sub('</a>','',str(i)) for i in datetime]\n",
    "datetime=[i[2:-2] for i in datetime]\n",
    "datetime=[re.sub('\\\\\\.*a0',' ',str(i)) for i in datetime]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "region=[re.sub('<td class=\"tb_region\" id=\"reg.*?>','',str(i)) for i in region]\n",
    "region=[re.sub('</td>','',str(i)) for i in region]\n",
    "region=[i[1:] for i in region]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "earthquakes=pd.DataFrame(data={'Date':datetime,'Longitud':longitud,'Latitud':latitud,'Region':region})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Time</th>\n",
       "      <th>Longitud</th>\n",
       "      <th>Latitud</th>\n",
       "      <th>Region</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-11-09</td>\n",
       "      <td>23:42:03.2</td>\n",
       "      <td>26.99 E</td>\n",
       "      <td>37.87 N</td>\n",
       "      <td>DODECANESE ISLANDS, GREECE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-11-09</td>\n",
       "      <td>23:30:29.0</td>\n",
       "      <td>127.64 E</td>\n",
       "      <td>2.97 S</td>\n",
       "      <td>CERAM SEA, INDONESIA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-11-09</td>\n",
       "      <td>23:24:38.7</td>\n",
       "      <td>3.69 W</td>\n",
       "      <td>35.47 N</td>\n",
       "      <td>STRAIT OF GIBRALTAR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-11-09</td>\n",
       "      <td>23:18:27.8</td>\n",
       "      <td>45.72 E</td>\n",
       "      <td>35.37 N</td>\n",
       "      <td>IRAN-IRAQ BORDER REGION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-11-09</td>\n",
       "      <td>23:16:03.4</td>\n",
       "      <td>26.61 E</td>\n",
       "      <td>37.88 N</td>\n",
       "      <td>DODECANESE ISLANDS, GREECE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2020-11-09</td>\n",
       "      <td>23:12:33.0</td>\n",
       "      <td>118.26 E</td>\n",
       "      <td>11.62 S</td>\n",
       "      <td>SOUTH OF SUMBAWA, INDONESIA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2020-11-09</td>\n",
       "      <td>23:02:38.0</td>\n",
       "      <td>70.76 W</td>\n",
       "      <td>25.25 S</td>\n",
       "      <td>OFFSHORE ANTOFAGASTA, CHILE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2020-11-09</td>\n",
       "      <td>22:42:58.0</td>\n",
       "      <td>69.90 W</td>\n",
       "      <td>31.90 S</td>\n",
       "      <td>SAN JUAN, ARGENTINA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2020-11-09</td>\n",
       "      <td>22:32:50.4</td>\n",
       "      <td>26.42 E</td>\n",
       "      <td>37.81 N</td>\n",
       "      <td>DODECANESE ISLANDS, GREECE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2020-11-09</td>\n",
       "      <td>22:24:51.1</td>\n",
       "      <td>117.87 W</td>\n",
       "      <td>38.17 N</td>\n",
       "      <td>NEVADA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2020-11-09</td>\n",
       "      <td>22:24:47.6</td>\n",
       "      <td>13.17 E</td>\n",
       "      <td>42.56 N</td>\n",
       "      <td>CENTRAL ITALY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2020-11-09</td>\n",
       "      <td>22:23:14.9</td>\n",
       "      <td>13.25 E</td>\n",
       "      <td>38.82 N</td>\n",
       "      <td>SICILY, ITALY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2020-11-09</td>\n",
       "      <td>22:18:47.0</td>\n",
       "      <td>27.01 E</td>\n",
       "      <td>37.88 N</td>\n",
       "      <td>WESTERN TURKEY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2020-11-09</td>\n",
       "      <td>22:16:27.7</td>\n",
       "      <td>26.99 E</td>\n",
       "      <td>37.86 N</td>\n",
       "      <td>DODECANESE ISLANDS, GREECE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2020-11-09</td>\n",
       "      <td>22:13:59.7</td>\n",
       "      <td>38.22 E</td>\n",
       "      <td>36.77 N</td>\n",
       "      <td>TURKEY-SYRIA BORDER REGION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2020-11-09</td>\n",
       "      <td>22:07:43.4</td>\n",
       "      <td>117.54 W</td>\n",
       "      <td>47.74 N</td>\n",
       "      <td>WASHINGTON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2020-11-09</td>\n",
       "      <td>22:01:35.5</td>\n",
       "      <td>159.94 W</td>\n",
       "      <td>54.16 N</td>\n",
       "      <td>SOUTH OF ALASKA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2020-11-09</td>\n",
       "      <td>21:58:29.3</td>\n",
       "      <td>150.99 W</td>\n",
       "      <td>60.62 N</td>\n",
       "      <td>KENAI PENINSULA, ALASKA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2020-11-09</td>\n",
       "      <td>21:56:05.7</td>\n",
       "      <td>66.94 W</td>\n",
       "      <td>17.83 N</td>\n",
       "      <td>PUERTO RICO REGION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2020-11-09</td>\n",
       "      <td>21:55:19.8</td>\n",
       "      <td>27.04 E</td>\n",
       "      <td>37.73 N</td>\n",
       "      <td>WESTERN TURKEY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2020-11-09</td>\n",
       "      <td>21:43:40.8</td>\n",
       "      <td>27.16 E</td>\n",
       "      <td>38.99 N</td>\n",
       "      <td>WESTERN TURKEY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2020-11-09</td>\n",
       "      <td>21:39:17.0</td>\n",
       "      <td>101.00 W</td>\n",
       "      <td>18.23 N</td>\n",
       "      <td>GUERRERO, MEXICO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2020-11-09</td>\n",
       "      <td>21:38:10.9</td>\n",
       "      <td>121.94 W</td>\n",
       "      <td>48.10 N</td>\n",
       "      <td>WASHINGTON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2020-11-09</td>\n",
       "      <td>21:33:57.1</td>\n",
       "      <td>15.64 E</td>\n",
       "      <td>38.67 N</td>\n",
       "      <td>SICILY, ITALY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2020-11-09</td>\n",
       "      <td>21:16:48.0</td>\n",
       "      <td>80.59 W</td>\n",
       "      <td>6.71 S</td>\n",
       "      <td>NEAR COAST OF NORTHERN PERU</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2020-11-09</td>\n",
       "      <td>21:14:34.8</td>\n",
       "      <td>27.00 E</td>\n",
       "      <td>37.88 N</td>\n",
       "      <td>WESTERN TURKEY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2020-11-09</td>\n",
       "      <td>21:12:46.7</td>\n",
       "      <td>26.94 E</td>\n",
       "      <td>37.86 N</td>\n",
       "      <td>DODECANESE ISLANDS, GREECE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>2020-11-09</td>\n",
       "      <td>21:07:20.6</td>\n",
       "      <td>152.81 W</td>\n",
       "      <td>60.32 N</td>\n",
       "      <td>SOUTHERN ALASKA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>2020-11-09</td>\n",
       "      <td>21:07:08.2</td>\n",
       "      <td>27.16 E</td>\n",
       "      <td>38.99 N</td>\n",
       "      <td>WESTERN TURKEY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>2020-11-09</td>\n",
       "      <td>21:04:42.8</td>\n",
       "      <td>27.14 E</td>\n",
       "      <td>39.01 N</td>\n",
       "      <td>WESTERN TURKEY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>2020-11-09</td>\n",
       "      <td>21:00:03.4</td>\n",
       "      <td>27.00 E</td>\n",
       "      <td>37.89 N</td>\n",
       "      <td>WESTERN TURKEY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>2020-11-09</td>\n",
       "      <td>20:56:08.4</td>\n",
       "      <td>122.82 W</td>\n",
       "      <td>38.81 N</td>\n",
       "      <td>NORTHERN CALIFORNIA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>2020-11-09</td>\n",
       "      <td>20:48:08.6</td>\n",
       "      <td>26.99 E</td>\n",
       "      <td>37.88 N</td>\n",
       "      <td>DODECANESE ISLANDS, GREECE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>2020-11-09</td>\n",
       "      <td>20:44:48.9</td>\n",
       "      <td>7.52 E</td>\n",
       "      <td>45.96 N</td>\n",
       "      <td>NORTHERN ITALY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>2020-11-09</td>\n",
       "      <td>20:42:09.0</td>\n",
       "      <td>70.54 W</td>\n",
       "      <td>31.72 S</td>\n",
       "      <td>COQUIMBO, CHILE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>2020-11-09</td>\n",
       "      <td>20:30:59.8</td>\n",
       "      <td>27.00 E</td>\n",
       "      <td>37.87 N</td>\n",
       "      <td>WESTERN TURKEY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>2020-11-09</td>\n",
       "      <td>20:30:57.1</td>\n",
       "      <td>116.24 E</td>\n",
       "      <td>19.96 S</td>\n",
       "      <td>NORTHWEST OF AUSTRALIA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>2020-11-09</td>\n",
       "      <td>20:26:05.2</td>\n",
       "      <td>161.84 E</td>\n",
       "      <td>55.78 N</td>\n",
       "      <td>NEAR EAST COAST OF KAMCHATKA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>2020-11-09</td>\n",
       "      <td>20:12:30.0</td>\n",
       "      <td>71.07 W</td>\n",
       "      <td>28.87 S</td>\n",
       "      <td>ATACAMA, CHILE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>2020-11-09</td>\n",
       "      <td>19:41:51.7</td>\n",
       "      <td>26.82 E</td>\n",
       "      <td>37.84 N</td>\n",
       "      <td>DODECANESE ISLANDS, GREECE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>2020-11-09</td>\n",
       "      <td>19:40:02.6</td>\n",
       "      <td>26.41 E</td>\n",
       "      <td>38.82 N</td>\n",
       "      <td>NEAR THE COAST OF WESTERN TURKEY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>2020-11-09</td>\n",
       "      <td>19:34:29.7</td>\n",
       "      <td>41.99 E</td>\n",
       "      <td>38.72 N</td>\n",
       "      <td>EASTERN TURKEY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>2020-11-09</td>\n",
       "      <td>19:13:15.0</td>\n",
       "      <td>69.19 W</td>\n",
       "      <td>28.33 S</td>\n",
       "      <td>LA RIOJA, ARGENTINA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>2020-11-09</td>\n",
       "      <td>19:08:41.6</td>\n",
       "      <td>66.37 W</td>\n",
       "      <td>22.65 S</td>\n",
       "      <td>JUJUY, ARGENTINA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>2020-11-09</td>\n",
       "      <td>18:56:20.4</td>\n",
       "      <td>26.86 E</td>\n",
       "      <td>37.78 N</td>\n",
       "      <td>DODECANESE ISLANDS, GREECE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>2020-11-09</td>\n",
       "      <td>18:54:17.4</td>\n",
       "      <td>26.91 E</td>\n",
       "      <td>37.83 N</td>\n",
       "      <td>DODECANESE ISLANDS, GREECE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>2020-11-09</td>\n",
       "      <td>18:30:54.2</td>\n",
       "      <td>9.13 E</td>\n",
       "      <td>46.90 N</td>\n",
       "      <td>SWITZERLAND</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>2020-11-09</td>\n",
       "      <td>18:29:03.0</td>\n",
       "      <td>70.13 W</td>\n",
       "      <td>18.96 N</td>\n",
       "      <td>DOMINICAN REPUBLIC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>2020-11-09</td>\n",
       "      <td>18:18:19.0</td>\n",
       "      <td>95.55 E</td>\n",
       "      <td>4.30 N</td>\n",
       "      <td>NORTHERN SUMATRA, INDONESIA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>2020-11-09</td>\n",
       "      <td>17:56:18.1</td>\n",
       "      <td>44.65 E</td>\n",
       "      <td>38.44 N</td>\n",
       "      <td>TURKEY-IRAN BORDER REGION</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Date        Time  Longitud  Latitud  \\\n",
       "0   2020-11-09  23:42:03.2   26.99 E  37.87 N   \n",
       "1   2020-11-09  23:30:29.0  127.64 E   2.97 S   \n",
       "2   2020-11-09  23:24:38.7    3.69 W  35.47 N   \n",
       "3   2020-11-09  23:18:27.8   45.72 E  35.37 N   \n",
       "4   2020-11-09  23:16:03.4   26.61 E  37.88 N   \n",
       "5   2020-11-09  23:12:33.0  118.26 E  11.62 S   \n",
       "6   2020-11-09  23:02:38.0   70.76 W  25.25 S   \n",
       "7   2020-11-09  22:42:58.0   69.90 W  31.90 S   \n",
       "8   2020-11-09  22:32:50.4   26.42 E  37.81 N   \n",
       "9   2020-11-09  22:24:51.1  117.87 W  38.17 N   \n",
       "10  2020-11-09  22:24:47.6   13.17 E  42.56 N   \n",
       "11  2020-11-09  22:23:14.9   13.25 E  38.82 N   \n",
       "12  2020-11-09  22:18:47.0   27.01 E  37.88 N   \n",
       "13  2020-11-09  22:16:27.7   26.99 E  37.86 N   \n",
       "14  2020-11-09  22:13:59.7   38.22 E  36.77 N   \n",
       "15  2020-11-09  22:07:43.4  117.54 W  47.74 N   \n",
       "16  2020-11-09  22:01:35.5  159.94 W  54.16 N   \n",
       "17  2020-11-09  21:58:29.3  150.99 W  60.62 N   \n",
       "18  2020-11-09  21:56:05.7   66.94 W  17.83 N   \n",
       "19  2020-11-09  21:55:19.8   27.04 E  37.73 N   \n",
       "20  2020-11-09  21:43:40.8   27.16 E  38.99 N   \n",
       "21  2020-11-09  21:39:17.0  101.00 W  18.23 N   \n",
       "22  2020-11-09  21:38:10.9  121.94 W  48.10 N   \n",
       "23  2020-11-09  21:33:57.1   15.64 E  38.67 N   \n",
       "24  2020-11-09  21:16:48.0   80.59 W   6.71 S   \n",
       "25  2020-11-09  21:14:34.8   27.00 E  37.88 N   \n",
       "26  2020-11-09  21:12:46.7   26.94 E  37.86 N   \n",
       "27  2020-11-09  21:07:20.6  152.81 W  60.32 N   \n",
       "28  2020-11-09  21:07:08.2   27.16 E  38.99 N   \n",
       "29  2020-11-09  21:04:42.8   27.14 E  39.01 N   \n",
       "30  2020-11-09  21:00:03.4   27.00 E  37.89 N   \n",
       "31  2020-11-09  20:56:08.4  122.82 W  38.81 N   \n",
       "32  2020-11-09  20:48:08.6   26.99 E  37.88 N   \n",
       "33  2020-11-09  20:44:48.9    7.52 E  45.96 N   \n",
       "34  2020-11-09  20:42:09.0   70.54 W  31.72 S   \n",
       "35  2020-11-09  20:30:59.8   27.00 E  37.87 N   \n",
       "36  2020-11-09  20:30:57.1  116.24 E  19.96 S   \n",
       "37  2020-11-09  20:26:05.2  161.84 E  55.78 N   \n",
       "38  2020-11-09  20:12:30.0   71.07 W  28.87 S   \n",
       "39  2020-11-09  19:41:51.7   26.82 E  37.84 N   \n",
       "40  2020-11-09  19:40:02.6   26.41 E  38.82 N   \n",
       "41  2020-11-09  19:34:29.7   41.99 E  38.72 N   \n",
       "42  2020-11-09  19:13:15.0   69.19 W  28.33 S   \n",
       "43  2020-11-09  19:08:41.6   66.37 W  22.65 S   \n",
       "44  2020-11-09  18:56:20.4   26.86 E  37.78 N   \n",
       "45  2020-11-09  18:54:17.4   26.91 E  37.83 N   \n",
       "46  2020-11-09  18:30:54.2    9.13 E  46.90 N   \n",
       "47  2020-11-09  18:29:03.0   70.13 W  18.96 N   \n",
       "48  2020-11-09  18:18:19.0   95.55 E   4.30 N   \n",
       "49  2020-11-09  17:56:18.1   44.65 E  38.44 N   \n",
       "\n",
       "                              Region  \n",
       "0         DODECANESE ISLANDS, GREECE  \n",
       "1               CERAM SEA, INDONESIA  \n",
       "2                STRAIT OF GIBRALTAR  \n",
       "3            IRAN-IRAQ BORDER REGION  \n",
       "4         DODECANESE ISLANDS, GREECE  \n",
       "5        SOUTH OF SUMBAWA, INDONESIA  \n",
       "6        OFFSHORE ANTOFAGASTA, CHILE  \n",
       "7                SAN JUAN, ARGENTINA  \n",
       "8         DODECANESE ISLANDS, GREECE  \n",
       "9                             NEVADA  \n",
       "10                     CENTRAL ITALY  \n",
       "11                     SICILY, ITALY  \n",
       "12                    WESTERN TURKEY  \n",
       "13        DODECANESE ISLANDS, GREECE  \n",
       "14        TURKEY-SYRIA BORDER REGION  \n",
       "15                        WASHINGTON  \n",
       "16                   SOUTH OF ALASKA  \n",
       "17           KENAI PENINSULA, ALASKA  \n",
       "18                PUERTO RICO REGION  \n",
       "19                    WESTERN TURKEY  \n",
       "20                    WESTERN TURKEY  \n",
       "21                  GUERRERO, MEXICO  \n",
       "22                        WASHINGTON  \n",
       "23                     SICILY, ITALY  \n",
       "24       NEAR COAST OF NORTHERN PERU  \n",
       "25                    WESTERN TURKEY  \n",
       "26        DODECANESE ISLANDS, GREECE  \n",
       "27                   SOUTHERN ALASKA  \n",
       "28                    WESTERN TURKEY  \n",
       "29                    WESTERN TURKEY  \n",
       "30                    WESTERN TURKEY  \n",
       "31               NORTHERN CALIFORNIA  \n",
       "32        DODECANESE ISLANDS, GREECE  \n",
       "33                    NORTHERN ITALY  \n",
       "34                   COQUIMBO, CHILE  \n",
       "35                    WESTERN TURKEY  \n",
       "36            NORTHWEST OF AUSTRALIA  \n",
       "37      NEAR EAST COAST OF KAMCHATKA  \n",
       "38                    ATACAMA, CHILE  \n",
       "39        DODECANESE ISLANDS, GREECE  \n",
       "40  NEAR THE COAST OF WESTERN TURKEY  \n",
       "41                    EASTERN TURKEY  \n",
       "42               LA RIOJA, ARGENTINA  \n",
       "43                  JUJUY, ARGENTINA  \n",
       "44        DODECANESE ISLANDS, GREECE  \n",
       "45        DODECANESE ISLANDS, GREECE  \n",
       "46                       SWITZERLAND  \n",
       "47                DOMINICAN REPUBLIC  \n",
       "48       NORTHERN SUMATRA, INDONESIA  \n",
       "49         TURKEY-IRAN BORDER REGION  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "earthquakes[['Date','Time']]=earthquakes['Date'].str.split(' ',expand = True)\n",
    "earthquakes=earthquakes[['Date', 'Time','Longitud', 'Latitud', 'Region']]\n",
    "earthquakes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Count the number of tweets by a given Twitter account.\n",
    "Ask the user for the handle (@handle) of a twitter account. You will need to include a ***try/except block*** for account names not found. \n",
    "<br>***Hint:*** the program should count the number of tweets for any provided account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss = \"WxC0WNbm1S12U3yKDMyVExYDUsYAkYuKxBDD8xoSWdxgv\"\n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, ss)\n",
    "api = tweepy.API(auth,wait_on_rate_limit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "id = 33906088\n",
    "\n",
    "try:\n",
    "    # fetching the user \n",
    "    user = api.get_user(id) \n",
    "  \n",
    "    # fetching the statuses_count attribute \n",
    "    statuses_count = user.statuses_count \n",
    "    \n",
    "except BaseException as e:\n",
    "      print('failed on_status,',str(e))\n",
    "      time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of statuses the user has posted are : 90165\n"
     ]
    }
   ],
   "source": [
    "  \n",
    "print(\"The number of statuses the user has posted are : \" + str(statuses_count)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Number of followers of a given twitter account\n",
    "Ask the user for the handle (@handle) of a twitter account. You will need to include a ***try/except block*** for account names not found. \n",
    "<br>***Hint:*** the program should count the followers for any provided account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2707112\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    print (user.followers_count)\n",
    "\n",
    "except BaseException as e:\n",
    "      print('failed on_status,',str(e))\n",
    "      time.sleep(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### List all language names and number of related articles in the order they appear in wikipedia.org."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://www.wikipedia.org/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "html = requests.get(url).content\n",
    "soup = BeautifulSoup(html, 'lxml')\n",
    "wiki=soup.find_all(\"a\", class_=\"link-box\")\n",
    "language_names=[re.findall('<strong>.*</strong>',str(i)) for i in wiki]\n",
    "numer_related_articles=[re.findall('ltr\">.*</bdi>',str(i)) for i in wiki]\n",
    "wikipedia=pd.DataFrame(data={'Language Names':language_names,'Number of Related Articles':numer_related_articles})\n",
    "wikipedia['Language Names']=wikipedia['Language Names'].apply(lambda x: re.sub('<strong>','',str(x)))\n",
    "wikipedia['Language Names']=wikipedia['Language Names'].apply(lambda x: re.sub('</strong>','',str(x)))\n",
    "wikipedia['Language Names']=wikipedia['Language Names'].apply(lambda x: x[2:-2])\n",
    "wikipedia['Number of Related Articles']=wikipedia['Number of Related Articles'].apply(lambda x: re.sub('[\\D+]','',str(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Language Names</th>\n",
       "      <th>Number of Related Articles</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>English</td>\n",
       "      <td>601830000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Español</td>\n",
       "      <td>106370000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>日本語</td>\n",
       "      <td>102350000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Deutsch</td>\n",
       "      <td>204950000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Русский</td>\n",
       "      <td>106720000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Français</td>\n",
       "      <td>202620000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Italiano</td>\n",
       "      <td>106450000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>中文</td>\n",
       "      <td>101550000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Português</td>\n",
       "      <td>100450000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Polski</td>\n",
       "      <td>104350000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Language Names Number of Related Articles\n",
       "0        English                  601830000\n",
       "1        Español                  106370000\n",
       "2            日本語                  102350000\n",
       "3        Deutsch                  204950000\n",
       "4        Русский                  106720000\n",
       "5       Français                  202620000\n",
       "6       Italiano                  106450000\n",
       "7             中文                  101550000\n",
       "8      Português                  100450000\n",
       "9         Polski                  104350000"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wikipedia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A list with the different kind of datasets available in data.gov.uk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://data.gov.uk/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Available Datasets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Business and economy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Crime and justice</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Defence</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Education</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Environment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Government</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Government spending</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Health</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Mapping</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Society</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Towns and cities</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Transport</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Available Datasets\n",
       "0   Business and economy\n",
       "1      Crime and justice\n",
       "2                Defence\n",
       "3              Education\n",
       "4            Environment\n",
       "5             Government\n",
       "6    Government spending\n",
       "7                 Health\n",
       "8                Mapping\n",
       "9                Society\n",
       "10      Towns and cities\n",
       "11             Transport"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# your code here\n",
    "html = requests.get(url).content\n",
    "soup = BeautifulSoup(html, 'lxml')\n",
    "govuk=soup.find_all(\"a\", class_=\"govuk-link\")\n",
    "gov=[re.findall('>.*</a',str(i)) for i in govuk]\n",
    "gov=[re.sub('>','',str(i)) for i in gov]\n",
    "gov=[re.sub('</a','',str(i)) for i in gov]\n",
    "gov=[i[2:-2] for i in gov[3:]]\n",
    "gov=pd.DataFrame(data={'Available Datasets':gov})\n",
    "gov"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display the top 10 languages by number of native speakers stored in a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://en.wikipedia.org/wiki/List_of_languages_by_number_of_native_speakers'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "html = requests.get(url).content\n",
    "soup = BeautifulSoup(html, 'lxml')\n",
    "languages=soup.find_all(\"td\")\n",
    "languages=[re.sub('<td>','',str(i)) for i in languages]\n",
    "languages=[re.sub('\\n</td>','',str(i)) for i in languages]\n",
    "languages=[re.sub('<a href=\"/wiki/.*title=','',str(i)) for i in languages]\n",
    "languages=[re.sub('</a>','',str(i)) for i in languages]\n",
    "ll =[languages[i:i + 6] for i in range(0, len(languages), 6)]\n",
    "columns=['Rank','Language','Speakers','% of World pop.','Language family','Branch']\n",
    "languages=pd.DataFrame(data=ll,columns=columns)\n",
    "languages['Language']=languages['Language'].apply(lambda x:re.findall('>.*',str(x)))\n",
    "languages['Language']=languages['Language'].apply(lambda x:re.sub('>','',str(x)))\n",
    "languages['Language']=languages['Language'].apply(lambda x:re.sub('<.*','',str(x)))\n",
    "languages['Language']=languages['Language'].apply(lambda x:x[2:-2])\n",
    "languages.drop(columns=['Language family','Branch'],inplace=True)\n",
    "\n",
    "languages=languages.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rank</th>\n",
       "      <th>Language</th>\n",
       "      <th>Speakers</th>\n",
       "      <th>% of World pop.</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Mandarin Chinese</td>\n",
       "      <td>918</td>\n",
       "      <td>11.92%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Spanish</td>\n",
       "      <td>480</td>\n",
       "      <td>5.99%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>English</td>\n",
       "      <td>379</td>\n",
       "      <td>4.92%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Hindustan</td>\n",
       "      <td>341</td>\n",
       "      <td>4.43%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Bengali</td>\n",
       "      <td>228</td>\n",
       "      <td>2.96%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>Portuguese</td>\n",
       "      <td>221</td>\n",
       "      <td>2.87%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>Russian</td>\n",
       "      <td>154</td>\n",
       "      <td>2.00%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>Japanese</td>\n",
       "      <td>128</td>\n",
       "      <td>1.66%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>Western Punja</td>\n",
       "      <td>92.7</td>\n",
       "      <td>1.20%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>Marathi</td>\n",
       "      <td>83.1</td>\n",
       "      <td>1.08%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Rank          Language Speakers % of World pop.\n",
       "0    1  Mandarin Chinese      918          11.92%\n",
       "1    2           Spanish      480           5.99%\n",
       "2    3           English      379           4.92%\n",
       "3    4         Hindustan      341           4.43%\n",
       "4    5           Bengali      228           2.96%\n",
       "5    6        Portuguese      221           2.87%\n",
       "6    7           Russian      154           2.00%\n",
       "7    8          Japanese      128           1.66%\n",
       "8    9     Western Punja     92.7           1.20%\n",
       "9   10           Marathi     83.1           1.08%"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "languages['% of World pop.']=languages['% of World pop.'].astype(float)\n",
    "languages['% of World pop.'] = languages['% of World pop.'].apply(lambda x: \"{0:.2f}%\".format(x * 1))\n",
    "languages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus\n",
    "#### Scrape a certain number of tweets of a given Twitter account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise \n",
    "# You will need to add the account credentials to this url\n",
    "url = 'https://twitter.com/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "username = 'VogueSpain'\n",
    "count = 150\n",
    "\n",
    "try:     \n",
    " # Creation of query method using parameters\n",
    " tweets = tweepy.Cursor(api.user_timeline,id=username).items(count)\n",
    " \n",
    " # Pulling information from tweets iterable object\n",
    " tweets_list = [[tweet.created_at, tweet.id, tweet.text] for tweet in tweets]\n",
    " \n",
    " # Creation of dataframe from tweets list\n",
    " # Add or remove columns as you remove tweet information\n",
    " tweets_df = pd.DataFrame(tweets_list,columns=['Date','Tweet ID','Tweet Content'])\n",
    "\n",
    "except BaseException as e:\n",
    "      print('failed on_status,',str(e))\n",
    "      time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Tweet ID</th>\n",
       "      <th>Tweet Content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-11-10 00:01:06</td>\n",
       "      <td>1325951579980263426</td>\n",
       "      <td>Qué significa tener un ciclo menstrual irregul...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-11-09 22:55:04</td>\n",
       "      <td>1325934964429434880</td>\n",
       "      <td>El estilo de la cantante Sade o la respuesta c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-11-09 22:35:06</td>\n",
       "      <td>1325929936666959875</td>\n",
       "      <td>Todo lo que tienes que saber sobre la cosmétic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-11-09 22:10:05</td>\n",
       "      <td>1325923641461235716</td>\n",
       "      <td>Irina Shayk también tiene “ese” bolso que sabe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-11-09 21:45:10</td>\n",
       "      <td>1325917371891134464</td>\n",
       "      <td>Una vez que empieces, no podrás dejarlo.\\nhttp...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Date             Tweet ID  \\\n",
       "0 2020-11-10 00:01:06  1325951579980263426   \n",
       "1 2020-11-09 22:55:04  1325934964429434880   \n",
       "2 2020-11-09 22:35:06  1325929936666959875   \n",
       "3 2020-11-09 22:10:05  1325923641461235716   \n",
       "4 2020-11-09 21:45:10  1325917371891134464   \n",
       "\n",
       "                                       Tweet Content  \n",
       "0  Qué significa tener un ciclo menstrual irregul...  \n",
       "1  El estilo de la cantante Sade o la respuesta c...  \n",
       "2  Todo lo que tienes que saber sobre la cosmétic...  \n",
       "3  Irina Shayk también tiene “ese” bolso que sabe...  \n",
       "4  Una vez que empieces, no podrás dejarlo.\\nhttp...  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display IMDB's top 250 data (movie name, initial release, director name and stars) as a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise \n",
    "url = 'https://www.imdb.com/chart/top'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "html = requests.get(url).content\n",
    "soup = BeautifulSoup(html, 'lxml')\n",
    "movie=soup.find_all(\"td\", class_=\"titleColumn\")\n",
    "movie_name=[re.findall('>.*</a',str(i)) for i in movie]\n",
    "initial_release=[re.findall('>.*</span',str(i)) for i in movie]\n",
    "director=[re.findall('title=.* \\(dir.', str(i)) for i in movie]\n",
    "rating=soup.find_all(\"td\",class_=\"ratingColumn imdbRating\")\n",
    "rating=[re.findall('>.*<',str(x)) for x in rating]\n",
    "IMDB=pd.DataFrame(data={'Movie Name':movie_name,'Initial Release':initial_release,'Director Name':director,'Stars':rating})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Movie Name</th>\n",
       "      <th>Initial Release</th>\n",
       "      <th>Director Name</th>\n",
       "      <th>Stars</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Cadena perpetua</td>\n",
       "      <td>1994</td>\n",
       "      <td>Frank Darabont</td>\n",
       "      <td>9.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>El padrino</td>\n",
       "      <td>1972</td>\n",
       "      <td>Francis Ford Coppola</td>\n",
       "      <td>9.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>El padrino: Parte II</td>\n",
       "      <td>1974</td>\n",
       "      <td>Francis Ford Coppola</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>El caballero oscuro</td>\n",
       "      <td>2008</td>\n",
       "      <td>Christopher Nolan</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12 hombres sin piedad</td>\n",
       "      <td>1957</td>\n",
       "      <td>Sidney Lumet</td>\n",
       "      <td>8.9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Movie Name Initial Release          Director Name Stars\n",
       "0        Cadena perpetua            1994        Frank Darabont    9.2\n",
       "1             El padrino            1972  Francis Ford Coppola    9.1\n",
       "2   El padrino: Parte II            1974  Francis Ford Coppola    9.0\n",
       "3    El caballero oscuro            2008     Christopher Nolan    9.0\n",
       "4  12 hombres sin piedad            1957          Sidney Lumet    8.9"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IMDB['Movie Name']=IMDB['Movie Name'].apply(lambda x: re.sub('</a','',str(x)))\n",
    "IMDB['Movie Name']=IMDB['Movie Name'].apply(lambda x: x[3:-2])\n",
    "IMDB['Initial Release']=IMDB['Initial Release'].apply(lambda x: re.sub('</span','',str(x)))\n",
    "IMDB['Initial Release']=IMDB['Initial Release'].apply(lambda x: x[4:-3])\n",
    "IMDB['Director Name']=IMDB['Director Name'].apply(lambda x: re.sub('title=','',str(x)))\n",
    "IMDB['Director Name']=IMDB['Director Name'].apply(lambda x: x[3:-7])\n",
    "IMDB['Stars']=IMDB['Stars'].apply(lambda x: re.sub('>','',str(x)))\n",
    "IMDB['Stars']=IMDB['Stars'].apply(lambda x: x[2:-3])\n",
    "IMDB.replace(['>', '<'],inplace=True )\n",
    "IMDB.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display the movie name, year and a brief summary of the top 10 random movies (IMDB) as a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is the url you will scrape in this exercise\n",
    "url = 'http://www.imdb.com/chart/top'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find the live weather report (temperature, wind speed, description and weather) of a given city."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://openweathermap.org/current\n",
    "city = input('Enter the city: ')\n",
    "url = 'http://api.openweathermap.org/data/2.5/weather?'+'q='+city+'&APPID=b35975e18dc93725acb092f7272cc6b8&units=metric'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find the book name, price and stock availability as a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise. \n",
    "# It is a fictional bookstore created to be scraped. \n",
    "url = 'http://books.toscrape.com/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
